---
title: "Predicting Housing Prices - Data Analysis Project"
author: "Aaron Blythe, ablythe; Josh Janda: joshlj2; Jeanette Pettibone, jgp4"
date: "7/21/2019"
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
editor_options: 
  chunk_output_type: inline
---


# Technical Requirements

You may need to install Cairo on your operating system to run this notebook. To learn more see the Cairo documentation at https://www.cairographics.org/download/.

```{r, message = FALSE, warning = FALSE}
if(!require(Cairo)) install.packages("Cairo", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(car)) install.packages("car", repos = "http://cran.us.r-project.org")
if(!require(nortest)) install.packages("nortest", repos = "http://cran.us.r-project.org")
library(readr)
library(ggplot2)
library(knitr)
library(tidyverse)
library(caret)
library(leaps)
library(car)
library(mice)
library(scales)
library(RColorBrewer)
library(plotly)
library(nortest)
library(lmtest)
```

# Introduction

## Predicting Housing Prices

The purpose of this project is to predict the price of houses in California in 1990 based on a number of possible location-based predictors, including latitude, longitude, and information about houses within a particular block.

While this project focuses on prediction we are fully aware and want you the reader to also be aware that housing prices have increased dramatically since 1990, when the data was collected. This model should not be used to predict the actual future. This is a purely academic endeavor to explore statistical prediction.

The goal of the project is to create the model that can best predict home prices in California given reasonable test/train splits in the data.

## California Housing Prices Dataset

We're using the California Housing Prices dataset from the following Kaggle site: https://www.kaggle.com/camnugent/california-housing-prices.  This data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data.  

We loaded `housing.csv` into R.

```{r}
housing_data = read_csv("housing.csv")
```

The dataset contains 20640 observations and 10 attributes (9 predictors and 1 response).  Below is a list of the variables with descriptions taken from the original Kaggle site given above.  

- `longitude`: A measure of how far west a house is; a higher value is farther west
- `latitude`: A measure of how far north a house is; a higher value is farther north
- `housing_median_age`: Median age of a house within a block; a lower number is a newer building
- `total_rooms`: Total number of rooms within a block
- `total_bedrooms`: Total number of bedrooms within a block
- `population`: Total number of people residing within a block
- `households`: Total number of households, a group of people residing within a home unit, for a block
- `median_income`: Median income for households within a block of houses (measured in tens of thousands of US Dollars)
- `ocean_proximity`: Location of the house w.r.t ocean/sea
- `median_house_value`: Median house value for households within a block (measured in US Dollars)

This dataset meets all of the stated criteria for the project including:

* A minimum 200 observations
* A numeric response variable: `median_house_value`
* At least one categorical predictor: `ocean_proximity`
* At least two numeric predictors: the remaining attributes

Let's look at a summary of each variable:

```{r}
summary(housing_data)
```

Note that the `total_bedrooms` variable has *207* NA values. We will address this issue in the Data Cleaning section in Methods.

Below is a visual representation of all data points in the dataset with `longitude` on the x-axis, `latitude` on the y-axis, and `median_house_value` shown in a color codes.

```{r cache=TRUE}
plot_map = ggplot(housing_data, 
                  aes(x = longitude, y = latitude, color = median_house_value, 
                      hma = housing_median_age, tr = total_rooms, tb = total_bedrooms,
                      hh = households, mi = median_income)) +
              geom_point(aes(size = population), alpha = 0.4) +
              xlab("Longitude") +
              ylab("Latitude") +
              ggtitle("Data Map - Longtitude vs Latitude and Associated Variables") +
              theme(plot.title = element_text(hjust = 0.5)) +
              scale_color_distiller(palette = "Paired", labels = comma) +
              labs(color = "Median House Value (in $USD)", size = "Population")
plot_map
```

We see that on average the houses nearest to the ocean tend to have higher median house values, whereas those inland have the lower median values.  This difference is  quite substantial and tells us that the variable `ocean_proximity` will likely play a large role in predicting median house value.  

It's worth noting that some ocean-adjacent locations, like the more isolated towns along California's northern most coast, have lower median house prices than other coastal housing, and some inland locations, like the towns near destinations such as Yosemite National Park and Lake Tahoe, do have higher median house prices than other inland housing.  Also interesting to note that the houses closest to large metropolitan areas such as San Francisco and Los Angeles have among the highest median house values, whereas the largely agricultural Central Valley has among the lowest.  This lets us know that we cannot rely on `ocean_proximity` alone to tell the whole story.

# Methods

## Data Cleaning

### Categorical Variables

Initial exploration of the data showed us that there were a few steps we needed to take to make the data more useable.  Firstly, we changed the categorical variable `ocean_proximity` from text-based to a factor variable.  

```{r}
housing_data$ocean_proximity = as.factor(housing_data$ocean_proximity)
levels(housing_data$ocean_proximity)
```

Taking a deeper dive into `ocean_proximity`, we see that the level `ISLAND` has a very low count compared to the other levels.

```{r}
ggplot(housing_data, aes(x = factor(ocean_proximity))) +
  geom_bar(stat = "count", color = "black", fill = "blue")
```

We looked at the total count of each level to get a better idea of the skew in `ocean_proximity`.

```{r}
summary(housing_data$ocean_proximity)
```

In fact, `ISLAND` only has five rows, where every other level has over 2,000 rows. Due to possible issues with fitting models, we decided to remove this level from `ocean_proximity`.  We accepted the risk that our model will less accurately predict the value of houses on islands.

```{r}
housing_data = housing_data[housing_data$ocean_proximity != "ISLAND", ]
```

### Missing Data

The other thing to consider is missing data.

```{r}
sum(is.na(housing_data))
total_bedrooms = housing_data$total_bedrooms
sum(is.na(total_bedrooms))
```

There are $`r sum(is.na(total_bedrooms))`$ observations with missing data for `total_bedrooms`.  One thing we can do to solve this issue of NA values in `total_bedrooms` is to impute data points. We first want to take a look at the distribution of this variable to see which imputation method will work best.

```{r cache=TRUE}
bedroom_mean = mean(housing_data$total_bedrooms, na.rm=TRUE)
bedroom_median = median(housing_data$total_bedrooms, na.rm=TRUE)
ggplot(housing_data, aes(x = total_bedrooms)) +
  geom_histogram(bins = 40, color = "black", fill = "blue") +
  geom_vline(aes(xintercept = bedroom_mean, color = "Mean"), lwd = 1.5) +
  geom_vline(aes(xintercept = bedroom_median, color = "Median"), lwd = 1.5) +
  xlab("Total Bedrooms") +
  ylab("Frequency") +
  ggtitle("Histogram of Total Bedrooms (noncontinuous variable)") +
  scale_color_manual(name = "Summary Stats", labels = c("Mean", "Median"), values = c("red", "green"))
```

Looking at the distribution of the data in the histogram above, we decided to use the median of the `total_bedrooms` variable for our imputation method.

```{r}
housing_data$total_bedrooms[is.na(housing_data$total_bedrooms)] = bedroom_median
```


### Post-Cleaning

Looking at the structure of the dataset after cleaning the data, we see that besides the one factor variable `ocean_proximity`, we have nine numeric variables, three of which are continuous (`longitude`, `latitude`, and `median_income`) and six of which are discrete (`housing_median_age`, `total_rooms`, `total_bedrooms`, `population`, `households`, and `median_house_value`).

```{r}
str(housing_data)
```

For a better sense of the distribution of the nine numeric variables, we looked at histograms for each of them.

```{r, fig.height=10, fig.width=14}
par(mfrow = c(3, 3))
hist(housing_data$longitude, breaks = 20, main = "longitude", border="darkorange", col="dodgerblue")
hist(housing_data$latitude, breaks = 20, main = "latitude", border="darkorange", col="dodgerblue")
hist(housing_data$housing_median_age, breaks = 20, main = "housing_median_age", border="darkorange", col="dodgerblue")
hist(housing_data$total_rooms, breaks = 20, main = "total_rooms", border="darkorange", col="dodgerblue")
hist(housing_data$total_bedrooms, breaks = 20, main = "total_bedrooms", border="darkorange", col="dodgerblue")
hist(housing_data$population, breaks = 20, main = "population", border="darkorange", col="dodgerblue")
hist(housing_data$households, breaks = 20, main = "households", border="darkorange", col="dodgerblue")
hist(housing_data$median_income, breaks = 20, main = "median_income", border="darkorange", col="dodgerblue")
hist(housing_data$median_house_value, breaks = 20, main = "median_house_value", border="darkorange", col="dodgerblue")
```

To better understand the relationship between the all the variables, we looked at the pairs.

```{r cache=TRUE}
pairs(housing_data, col = "dodgerblue")
```

It looks like there are some additional variables which may not be necessary due to collinearity.  We looked further into the correlations between the numeric variables and showed them in the table below: 

```{r}
housing_data_nc = housing_data[, -10]

corrmatrix = cor(housing_data_nc)

kable(t(corrmatrix))
```

One thing that sticks out is the high collinearity between `households` and `total_bedrooms`, as well as `households` and `total_rooms`. While these variables are highly correlated, we believe it is best to leave them in as they are influential in the pricing of homes typically. Overall, we do not believe that collinearility will play a substantial role in influencing our models so we will be keeping all variables.


## Training and Test Data

First we want to split the data into training and testing data. We will use an 70/30 split of a randomized sample. We will use a set seed to get repeatability.

Note that we decided to partition using `ocean_proximity` since this is a predictor that we believe will play a large role in predicting housing prices.

```{r}
set.seed(420)
housing_trn_idx = createDataPartition(housing_data$ocean_proximity, p = .70, list = FALSE)
housing_trn_data = housing_data[housing_trn_idx, ]
housing_tst_data = housing_data[-housing_trn_idx, ]
```


## Model Selection

### Using all Model Subsets

As a starting point we want to get an idea which parameters are good ones to use in a potential model. 

We can use `leaps` to tell us the "best" model, which is the one with the lowest Adjusted $R^2$ for each number of parameters. Through testing we found this to select the full additive model when given the full additive as a starting point. It is more interesting when we use the full additive model with all two-way interactions.

```{r}
regsubsets.out = regsubsets(median_house_value ~ (longitude + latitude + housing_median_age + total_rooms + population + median_income + ocean_proximity) ^ 2,
               data = housing_trn_data,
               nbest = 1,     # 1 best model for each number of predictors
               nvmax = 10,    # NULL for no limit on number of variables
               force.in = NULL, force.out = NULL, method = "exhaustive", really.big=T)

fit_all_sum = summary(regsubsets.out)

# Reference - R for Statistical Learning - David Dalpiaz
# https://daviddalpiaz.github.io/r4sl/subset-selection.html
par(mfrow = c(2, 2))
plot(fit_all_sum$rss, xlab = "Number of Variables", ylab = "RSS", type = "b")

plot(fit_all_sum$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "b")
best_adj_r2 = which.max(fit_all_sum$adjr2)
points(best_adj_r2, fit_all_sum$adjr2[best_adj_r2],
       col = "red",cex = 2, pch = 20)
text(fit_all_sum$adjr2, labels=rownames(fit_all_sum$adjr2),data=fit_all_sum, cex=0.9, font=2)

plot(fit_all_sum$cp, xlab = "Number of Variables", ylab = "Cp", type = 'b')
best_cp = which.min(fit_all_sum$cp)
points(best_cp, fit_all_sum$cp[best_cp], 
       col = "red", cex = 2, pch = 20)

plot(fit_all_sum$bic, xlab = "Number of Variables", ylab = "BIC", type = 'b')
best_bic = which.min(fit_all_sum$bic)
points(best_bic, fit_all_sum$bic[best_bic], 
       col = "red", cex = 2, pch = 20)
```

The largest model works out to be the best model in all cases of RSS, Adjusted $R^2$, Cp, and BIC. It starts to flatten out after 10 predictors. We did the same leaps test with up to 20 predictors and received similar results as with 10, however the processing of the model creation and run took quite a bit longer, so we chose to stop at 10.

From this information we will extract the top 3 models that represent the best model of all the models with 10 predictors, the best model of all the models with 9 predictors and the best model of all the models with 8 predictors.

We will start with the best model of all the models with 10 predictors.

```{r}
coef(regsubsets.out, 10)
```

The first model to explore, `best_exhaustive_model_1`, uses the following predictors:

- `longitude`
- `latitude`
- `I(ocean_proximity == \"INLAND\")TRUE`
- `I(ocean_proximity == \"NEAR BAY\")TRUE`
- `longitude:latitude`
- `longitude:median_income`
- `longitude:I(ocean_proximity == \"NEAR BAY\")TRUE`
- `housing_median_age:total_rooms`
- `housing_median_age:population`
- `median_income:population`

```{r}
best_exhaustive_model_1 = lm(median_house_value ~ longitude + latitude + I(ocean_proximity == "INLAND") + I(ocean_proximity == "NEAR BAY") + longitude:latitude + longitude:median_income + longitude:I(ocean_proximity == "NEAR BAY") + housing_median_age:total_rooms + housing_median_age:population + population:median_income, data = housing_trn_data)
summary(best_exhaustive_model_1)$"adj.r.squared"
#names(coef(best_exhaustive_model_1))[-1]
```

This is an interesting set of predictors and they make a lot of sense. As we saw in the graph in the Dataset section titled `Data Map - Longtitude vs Latitude and Associated Variables`, it looks like housing prices are influenced by large metropolitan areas such as San Francisco and Los Angeles, where `longitude`, `latitude` and `ocean_proximity` would all play a large role.  Also, `median_income` may also be a proxy for the larger cities versus the more rural areas.

The second model is explore, `best_exhaustive_model_2`, with 9 predictors.

```{r}
coef(regsubsets.out, 9)
```

The `best_exhaustive_model_2` uses the following predictors:

- `housing_median_age`
- `I(ocean_proximity == \"INLAND\")TRUE`
- `I(ocean_proximity == \"NEAR BAY\")TRUE`
- `housing_median_age:longitude`
- `longitude:median_income`
- `I(ocean_proximity == \"NEAR BAY\")TRUE:longitude`
- `housing_median_age:total_rooms`
- `housing_median_age:population`

```{r}
best_exhaustive_model_2 = lm(median_house_value ~ housing_median_age + I(ocean_proximity == "INLAND") + I(ocean_proximity == "NEAR BAY") + longitude:housing_median_age + longitude:median_income + longitude:I(ocean_proximity == "NEAR BAY") + housing_median_age:total_rooms + housing_median_age:population, data = housing_trn_data)
summary(best_exhaustive_model_2)$"adj.r.squared"
#names(coef(best_exhaustive_model_2))[-1]
```

We see in this model, `housing_median_age` seems much more influential than in `best_exhaustive_model_1`.  The `ocean_proximity`, however, is still quite important here.

The third model we'll explore is the one with 8 predictors.

```{r}
coef(regsubsets.out, 8)
```

The `best_exhaustive_model_3` uses the following predictors:

- `median_income`
- `I(ocean_proximity == \"INLAND\")TRUE`
- `I(ocean_proximity == \"NEAR BAY\")TRUE`
- `median_income:longitude`
- `median_income:latitude`
- `I(ocean_proximity == \"NEAR BAY\")TRUE:longitude`
- `housing_median_age:total_rooms`
- `housing_median_age:population`

```{r}
best_exhaustive_model_3 = lm(median_house_value ~ median_income + I(ocean_proximity == "INLAND") + I(ocean_proximity == "NEAR BAY") + longitude:median_income + latitude:median_income + longitude:I(ocean_proximity == "NEAR BAY") + housing_median_age:total_rooms + housing_median_age:population, data = housing_trn_data)
summary(best_exhaustive_model_3)$"adj.r.squared"
#names(coef(best_exhaustive_model_3))[-1]
```



This model is very similar to the `best_exhaustive_model_2`, except that `median_income` is more important here.

We will also include a fourth model, `full_twoway_model`, which includes all variables in the model as well as their two-way interactions.

```{r}
full_twoway_model = twoway_mod_start = lm(median_house_value ~ (.)^2, data = housing_trn_data)
twoway_adjr2 = summary(full_twoway_model)$adj.r.squared
```

This model has an adjusted $R^2$ of $`r twoway_adjr2`$. Comparing the adjusted $R^2$ of all four models, we can see that this full two-way interaction model scores the highest which gives us reason to believe this model may be the best performer. In order to test this, we will do some more model testing and building below starting with looking at AIC performance.

We can now compare the AIC of these four models.

```{r}
aic_beginning_mods = data.frame(
                                "Total Predictors" = c("Leaps Model 1" = extractAIC(best_exhaustive_model_1)[1],
                                                       "Leaps Model 2" = extractAIC(best_exhaustive_model_2)[1],
                                                       "Leaps Model 3" = extractAIC(best_exhaustive_model_3)[1],
                                                       "Two-Way Int. Model" = extractAIC(full_twoway_model)[1]
                                                       ),
                                "AIC" = c(
                                          "Leaps Model 1" = extractAIC(best_exhaustive_model_1)[2],
                                          "Leaps Model 2" = extractAIC(best_exhaustive_model_2)[2],
                                          "Leaps Model 3" = extractAIC(best_exhaustive_model_3)[2],
                                          "Two-Way Int. Model" = extractAIC(full_twoway_model)[2]
                                )
                              )
kable(aic_beginning_mods, align = c("c", "r"))
```

We see that the model with the best (i.e., lowest) AIC is `full_twoway_model`, with a score of $`r extractAIC(full_twoway_model)[2]`$.  However, although this model has the lowest AIC, it also has far more predictors (and therefore is more complex) than the other three models - $`r extractAIC(full_twoway_model)[1]`$ compared to $`r extractAIC(best_exhaustive_model_1)[1]`$ predictors for `best_exhaustive_model_1`, $`r extractAIC(best_exhaustive_model_2)[1]`$ predictors for `best_exhaustive_model_2` and $`r extractAIC(best_exhaustive_model_3)[1]`$ predictors for `best_exhaustive_model_3`.  This is just something to keep in mind as we move forward.

### Using AIC and BIC

All 4 of these models are good candidates for stepwise and backward AIC and BIC.

```{r}
# full_twoway_model
# AIC
back_twoway_mod_finish_aic = step(full_twoway_model, direction = "backward", trace = 0)
both_twoway_mod_finish_aic = step(full_twoway_model, direction = "both", trace = 0)
# BIC
n = length(resid(full_twoway_model))
back_twoway_mod_finish_bic = step(full_twoway_model, direction = "backward", k = log(n), trace = 0)
both_twoway_mod_finish_bic = step(full_twoway_model, direction = "both", k = log(n), trace = 0)

# best_exhaustive_model_1
# AIC
back_aic_mod_1 = step(best_exhaustive_model_1, direction = "backward", trace = 0)
both_aic_mod_1 = step(best_exhaustive_model_1, direction = "both", trace = 0)
# BIC
n = length(resid(best_exhaustive_model_1))
back_bic_mod_1 = step(best_exhaustive_model_1, direction = "backward", k = log(n), trace = 0)
both_bic_mod_1 = step(best_exhaustive_model_1, direction = "both", k = log(n), trace = 0)

# best_exhaustive_model_2
# AIC
back_aic_mod_2 = step(best_exhaustive_model_2, direction = "backward", trace = 0)
both_aic_mod_2 = step(best_exhaustive_model_2, direction = "both", trace = 0)
# BIC
n = length(resid(best_exhaustive_model_2))
back_bic_mod_2 = step(best_exhaustive_model_2, direction = "backward", k = log(n), trace = 0)
both_bic_mod_2 = step(best_exhaustive_model_2, direction = "both", k = log(n), trace = 0)

# best_exhaustive_model_3
# AIC
back_aic_mod_3 = step(best_exhaustive_model_3, direction = "backward", trace = 0)
both_aic_mod_3 = step(best_exhaustive_model_3, direction = "both", trace = 0)
# BIC
n = length(resid(best_exhaustive_model_3))
back_bic_mod_3 = step(best_exhaustive_model_3, direction = "backward", k = log(n), trace = 0)
both_bic_mod_3 = step(best_exhaustive_model_2, direction = "both", k = log(n), trace = 0)
```

```{r}
aic_and_bic_results = data.frame(
  "AIC" =
    c("Backward" =
        c("Two-Way" = extractAIC(back_twoway_mod_finish_aic)[2],
          "Exhaustive M1" = extractAIC(back_aic_mod_1)[2],
          "Exhaustive M2" = extractAIC(back_aic_mod_2)[2],
          "Exhaustive M3" = extractAIC(back_aic_mod_3)[2]),
      "Both" =
        c("Two-Way" = extractAIC(both_twoway_mod_finish_aic)[2],
          "Exhaustive M1" = extractAIC(both_aic_mod_1)[2],
          "Exhaustive M2" = extractAIC(both_aic_mod_2)[2],
          "Exhaustive M3" = extractAIC(both_aic_mod_3)[2])),
  "BIC" =
    c("Backward" =
        c("Two-Way" = extractAIC(back_twoway_mod_finish_bic)[2],
          "Exhaustive M1" = extractAIC(back_bic_mod_1)[2],
          "Exhaustive M2" = extractAIC(back_bic_mod_2)[2],
          "Exhaustive M3" = extractAIC(back_bic_mod_3)[2]),
      "Both" =
        c("Two-Way" = extractAIC(both_twoway_mod_finish_bic)[2],
          "Exhaustive M1" = extractAIC(both_bic_mod_1)[2],
          "Exhaustive M2" = extractAIC(both_bic_mod_2)[2],
          "Exhaustive M3" = extractAIC(both_bic_mod_3)[2])))

kable(aic_and_bic_results)
```

The exhaustive models that we started with (`best_exhaustive_model_1`, `best_exhaustive_model_2`, and `best_exhaustive_model_3`) did not improve significantly.  Neither did `full_twoway_model` which started with a full two-way interaction. However the `back_twoway_mod_finish_aic` model did have slight improvement over `full_twoway_model`, which was originally the best performing model. 


TODOs Jeanette:
- Not choosing the ones coming out of the full twoway model - we can tie this maybe to the discussion of the first extractAIC and that this model is far more complex and therefore might not be ideal despite its lower AIC??

We now have the four models `best_exhaustive_model_1`, `back_aic_mod_2` and `best_exhaustive_model_3` to test going forward. Let's check the assumptions on these three models.

```{r}
diagnostics = function(model, alpha = .05, pointcol = "orange", linecol = "blue", plots = TRUE, tests = TRUE, pointtype = 16) {
	if (plots == TRUE) {
		par(mfrow = c(1, 3))
		plot(
				fitted(model),
				resid(model),
				pch = pointtype,
				xlab = "Fitted Values",
				ylab = "Residuals",
				main = "Fitted vs Residuals",
				col = pointcol
			)
		abline(h = 0, lwd = 2, col = linecol)
		
		qqnorm(
				resid(model),
				pch = pointtype,
				main = "QQNorm Plot",
				col = pointcol
			)
		qqline(
				resid(model),
				lwd = 2,
				col = linecol
				)
		hist(
		    resid(model),
		    main = "Histogram of Residuals",
		    col = pointcol,
		    xlab = "Residuals",
		    ylab = "Frequency"
		    )
	}
	if (tests == TRUE) {
		ad_test = ad.test(resid(model))
		bp_test = bptest(model)
		test_results = data.frame(
									"Anderson-Darling Normality Test" = c("Test Statistic" = round(ad_test$statistic, 5),
															"P-Value" = ad_test$p.value,
															"Result" = ifelse(ad_test$p.value < alpha, "Reject", "Fail To Reject")
															),
									"Breusch-Pagan Test" = c("Test Statistic" = round(bp_test$statistic, 5),
															"P-Value" = bp_test$p.value,
															"Result" = ifelse(bp_test$p.value < alpha, "Reject", "Fail To Reject")
															)
									)
		kable(t(test_results), col.names = c("Test Statistic", "P-Value", "Decision"))
	}
}
```


```{r}
diagnostics(back_twoway_mod_finish_aic)
diagnostics(best_exhaustive_model_1)
diagnostics(back_aic_mod_2)
diagnostics(best_exhaustive_model_3)
```

TODO:
- Discuss the output of the diagnostics plot. (not having heteroskedasticity we likely should not use for inference and only for prediction.)
- TODO: try out log(y)/log response


## Detect Overfitting

```{r}
# From the text: http://daviddalpiaz.github.io/appliedstats/variable-selection-and-model-building.html
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```

TODO: use the models from the diagnostics above

```{r}
calc_loocv_rmse(best_exhaustive_model_1)
calc_loocv_rmse(best_exhaustive_model_3)
calc_loocv_rmse(back_aic_mod_2)
```

The `best_exhaustive_model_1` model has the lowest loocv_rmse.

TODOs:
- Explain what this means and why it matters.



# Results

## Make predictions

TODO: use the models from the diagnostics above
TODO: make this a table that is more readable

```{r}
# the actual median house values from the test set
actual = housing_tst_data$median_house_value

# predict using the best_exhaustive_model_1
predicted = predict(best_exhaustive_model_1, newdata = housing_tst_data)
(mod1_error = 100 * mean((abs(actual - predicted)) / actual))

# predict using the best_exhaustive_model_3
predicted = predict(best_exhaustive_model_3, newdata = housing_tst_data)
(mod2_error = 100 * mean((abs(actual - predicted)) / actual))

# predict using the back_aic_mod_2
predicted = predict(back_aic_mod_2, newdata = housing_tst_data)
(mod3_error = 100 * mean((abs(actual - predicted)) / actual))

errors = sort(c(mod1_error, mod2_error, mod3_error))
```

These three models have very similar percentages of error, ranging from $`r errors[1]`$% to $`r errors[3]`$%.

# Discussion

